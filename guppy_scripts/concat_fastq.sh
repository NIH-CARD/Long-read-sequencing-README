#!/bin/bash
# join fastqs as generated by GPU+MIG based guppy runs for Long read sequencing data
# Please note that final fastq for LR datae turns out to be > 100 GB, therefore it is recommende that a VM with reasonably large RAM should be used. WE USED VM with 128GB RAM.
i=0
cwd=$(echo "$PWD")
echo current working directory is "$cwd"
#PLEASE ADJUST THIS PATH ACCORDINGLY

for d in ../res_*; #*.fastq.gz;
do
        ((i=i+1))
	#PLEAE MAKE SURE THAT "sample" IS PROPERLY EXTRACTED
        sample=$(echo "$d" | cut -d'/' -f2|cut -d'_' -f 2-)
        echo got sample number "$i" and sample "$sample"
        cd "$d"
	
	#GO THROUGH ALL run (as generated by GPU+MIG based guppy) folders	
        for run in run*;
        do
	    runi=$(echo "$run"|rev|cut -d"_" -f 1 | rev)
            echo calling concat for pass folder in "$run" and run "$runi"
            
	    mkdir -p all_pass_fastq
            python "$cwd"/concat_fastq.py "$run"/pass/ all_pass_fastq/"$runi"_"$sample"_pass_joined.fastq.gz
            echo now calling concat for fail folder in "$run" and run "$runi"
	    mkdir -p all_fail_fastq
            python "$cwd"/concat_fastq.py "$run"/fail/ all_fail_fastq/"$runi"_"$sample"_fail_joined.fastq.gz
        done

	echo finally concatenating all joined from all pass run into single fastq file
        python "$cwd"/concat_fastq.py all_pass_fastq/ "$sample"_pass_joined.fastq.gz
        echo finally concatenating all joined from all fail run into single fastq file
        python "$cwd"/concat_fastq.py all_fail_fastq/ "$sample"_fail_joined.fastq.gz

	cd "$cwd"
done

#sudo shutdown
